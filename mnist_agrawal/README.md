# MNIST Agrawal

This MNIST digit classification moves us on from `mnist_weisberg` which was the simplest in-line Python code, to a very similar implementation with the NN layers defined within Python Classes.

With these class definitions the network can be more abstractly declared as below:
```
net_agrawal = [
    Dense(784,100),
    ReLU(),
    Dense(100,200),
    ReLU(),
    Dense(200,10)
]
```

Other network designs can be tested, for example:
```
net_lewis = [
    Dense(784,100),
    ReLU(),
    Dense(100,10)
]
```

## Loading the model

```
cd nn_intro
source venv/bin/activate
cd mnist_agrawal
python
```
Then in python:
```
exec(open("mnist_agrawal.py").read())
```

Should load the training dataset of MNIST images and output the following:
```
X_train is (50000, 784)
X_val is (10000, 784)
X_test is (10000, 784)
X_test[0] is:
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000      000000000000000000000000000000000000
000000000000                              00000000000000
00000000000000000000                        000000000000
00000000000000000000000000000000000000    00000000000000
00000000000000000000000000000000000000    00000000000000
000000000000000000000000000000000000    0000000000000000
0000000000000000000000000000000000      0000000000000000
0000000000000000000000000000000000    000000000000000000
00000000000000000000000000000000      000000000000000000
00000000000000000000000000000000    00000000000000000000
00000000000000000000000000000000    00000000000000000000
000000000000000000000000000000    0000000000000000000000
0000000000000000000000000000      0000000000000000000000
00000000000000000000000000      000000000000000000000000
00000000000000000000000000    00000000000000000000000000
000000000000000000000000    0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000    000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
>>>
```

## Training the model
In python:
```
train(net_agrawal)
```
Will output
```
Epoch 0
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1562/1562 [00:03<00:00, 451.83it/s]
Train accuracy: 0.9504
Val accuracy: 0.952
```
...continuing to:
```
Epoch 7
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1562/1562 [00:03<00:00, 460.94it/s]
Train accuracy: 0.99404
Val accuracy: 0.9757
>>>
```

## Test prediction for a single image
For example to test the first image in the `X_test` data:
```
infer(net_agrawal,X_test[0])
```
Gives the output:
```
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000      000000000000000000000000000000000000
000000000000                              00000000000000
00000000000000000000                        000000000000
00000000000000000000000000000000000000    00000000000000
00000000000000000000000000000000000000    00000000000000
000000000000000000000000000000000000    0000000000000000
0000000000000000000000000000000000      0000000000000000
0000000000000000000000000000000000    000000000000000000
00000000000000000000000000000000      000000000000000000
00000000000000000000000000000000    00000000000000000000
00000000000000000000000000000000    00000000000000000000
000000000000000000000000000000    0000000000000000000000
0000000000000000000000000000      0000000000000000000000
00000000000000000000000000      000000000000000000000000
00000000000000000000000000    00000000000000000000000000
000000000000000000000000    0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000    000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
-2.420 -0.905  2.276  7.042 -10.006 -3.893 -12.091  13.861 -2.286  5.661
SEVEN
>>>
```
The row of ten floating point numbers contains the learned activations for the ten digit classes 0 through 9.

## Test predictions for MNIST digits until wrong prediction
```
test(net_agrawal)
```
Will result in output:
```
Image 0 predict [7] vs 7
```
... continuing with more successful results for following images until a wrong prediction is made, for example:
```
Image 115 predict [9] vs 4
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000  0000000000000000000000
000000000000000000000000000000    0000000000000000000000
0000000000000000000000000000    000000000000000000000000
00000000000000000000000000    00000000000000000000000000
000000000000000000000000    0000000000000000000000000000
0000000000000000000000      0000000000000000000000000000
0000000000000000000000    00000000  00000000000000000000
00000000000000000000    0000000000    000000000000000000
000000000000000000    0000000000      000000000000000000
000000000000000000    0000000000    00000000000000000000
000000000000000000    0000000000    00000000000000000000
000000000000000000      000000      00000000000000000000
00000000000000000000              0000000000000000000000
0000000000000000000000            0000000000000000000000
0000000000000000000000000000    000000000000000000000000
0000000000000000000000000000    000000000000000000000000
00000000000000000000000000      000000000000000000000000
00000000000000000000000000      000000000000000000000000
00000000000000000000000000    00000000000000000000000000
00000000000000000000000000    00000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000
-4.750 -1.069 -3.576 -6.696  8.516 -3.871  1.028 -1.407 -1.988  11.341
NINE
>>>
```
Here you can see the FOUR was wrongly predicted as a NINE, but the FOUR was the next best guess.

## Future improvements

The `train()` function should be passed the data arrays to train on, rather than being hardcoded to use the `X_train`, `y_train`, `X_val`, `y_val` globals.

The softmax/cross-entropy code is embedded as function calls in the train function, this might be better provided as a Layer class.

Having implemented the later Conv layers in Python (see mnist_conv), it would probably be better to consistently use the images
as count x 28x28x1 rather than count x (784,). This would mean bringing forward the (simple) `Flatten` layer from mnist_conv into
this example.
